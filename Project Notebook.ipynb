{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "dataload.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_object(filename):\n",
    "    obj = {}\n",
    "    with open(filename, 'rb') as input:\n",
    "        obj = pickle.load(input)\n",
    "    return obj\n",
    "\n",
    "# process the dataset from raw file to DataFrame\n",
    "# returns df\n",
    "def init_dataset(filename='kddcup.data.txt'):\n",
    "    # read the CSV without header\n",
    "    # (discard row 0 if it contains the header)\n",
    "    # fix the malformed row 4817100 by removing columns 0:14\n",
    "    df = pd.read_csv(filename, error_bad_lines=False, header=None, engine='c', memory_map=True)\n",
    "    df1 = pd.read_csv(filename, header=None, skiprows=4817100-1, nrows=1, engine='c', memory_map=True).iloc[:, 14:]\n",
    "    #df1.columns = df.columns\n",
    "    df = df.append(df1)\n",
    "    if df.iloc[0, 0] == 'duration':\n",
    "        df = df[1:]\n",
    "    del(df1)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # read in the headers (exclude first row)\n",
    "    header = open('kddcup.names').readlines()[1:]\n",
    "    header = [d.split(':')[0] for d in header]\n",
    "\n",
    "    # set the column names on the DataFrame\n",
    "    df.columns = header + ['attack']\n",
    "\n",
    "    # remove trailing '.' in the attack labels\n",
    "    df['attack'] = df['attack'].str.slice(0, -1)\n",
    "\n",
    "    # add new column 'attack_type' containing\n",
    "    #     dos, u2r, r2l, probe, normal\n",
    "    attack_types = [d.split() for d in open('training_attack_types.txt').readlines()[:-1]]\n",
    "    attack_types.append(['normal', 'normal'])\n",
    "    attack_types = np.array(attack_types)\n",
    "    attack_types = pd.DataFrame({ 'attack_type' : attack_types[:,1] },\n",
    "        index=attack_types[:,0])\n",
    "    df['attack_type'] = attack_types.loc[df.attack].attack_type.values\n",
    "    return df\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def make_data():\n",
    "    df = init_dataset()\n",
    "\n",
    "    # the processed dataset is now in DataFrame 'df'.\n",
    "    # we first split it into train/test, before doing any analysis\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df.iloc[:, :-2], df.iloc[:, -2:], test_size=0.1, random_state=4129)\n",
    "\n",
    "    save_object([x_train, y_train], 'train.dat')\n",
    "    save_object([x_test, y_test], 'test.dat')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "make_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature transform"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# uses the imbalanced-learn library\n",
    "# http://contrib.scikit-learn.org/imbalanced-learn/stable/over_sampling.html\n",
    "# install: conda install -c conda-forge imbalanced-learn\n",
    "\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler #, SMOTE, ADASYN\n",
    "# from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "\n",
    "from dataload import load_object, save_object\n",
    "x_train, y_train = load_object('train.dat')\n",
    "x_test, y_test = load_object('test.dat')\n",
    "\n",
    "# ? it only takes numeric features ?\n",
    "# categoricals have to be transformed using one-hot encoding\n",
    "\n",
    "# other samplers to try\n",
    "# x_train_r, y_train_r = SMOTE(random_state=4129).fit_sample(x_train, y_train)\n",
    "# x_train_r, y_train_r = ADASYN(random_state=4129).fit_sample(x_train, y_train)\n",
    "\n",
    "def make_pipe(kcount=100000, levels=['smurf', 'neptune', 'normal']):\n",
    "    '''Make a pipeline of an under- and an over-sampler, that produces\n",
    "    kcount samples for each target classself.\n",
    "    levels: a list of target classes to under-sample.'''\n",
    "\n",
    "    ratio = dict(zip(levels, [kcount] * len(levels)))\n",
    "    # ratio = {'smurf':kcount, 'neptune':kcount, 'normal':kcount}\n",
    "\n",
    "    # down-sample majority classes to kcount\n",
    "    rus = RandomUnderSampler(random_state=4129, ratio=ratio)\n",
    "\n",
    "    # up-sample the others to kcount\n",
    "    ros = RandomOverSampler(random_state=4129)\n",
    "\n",
    "    from imblearn.pipeline import Pipeline\n",
    "    return Pipeline([('under',rus), ('over',ros)])\n",
    "\n",
    "# Counter(y_train.attack[:120])\n",
    "# Counter(y_train.attack_type[:1000])\n",
    "# Counter(y_train.attack)\n",
    "# Counter(y_train.attack_type)\n",
    "\n",
    "# make a sampling pipeline on the full training set (target = attack)\n",
    "samp_pipe = make_pipe(20000, levels=['smurf', 'neptune', 'normal'])\n",
    "x_train_r, y_train_r = samp_pipe.fit_sample(x_train.iloc[:, 4:], y_train.attack)\n",
    "\n",
    "# make a sampling pipeline on the full training set (target = attack_type)\n",
    "samp_pipe = make_pipe(20000, levels=['dos', 'normal', 'probe'])\n",
    "x_train_r, y_train_r = samp_pipe.fit_sample(x_train.iloc[:, 4:], y_train.attack_type)\n",
    "\n",
    "# x_train_r, y_train_r now contain the training set with balanced classes.\n",
    "# use them for modelling.\n",
    "\n",
    "save_object([x_train_r, y_train_r], 'train_r.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_r, y_train = load_object('train_r.dat')\n",
    "x_test_r, y_test = load_object('test.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 37)\n",
      "(100000,)\n",
      "(489844, 41)\n",
      "(489844, 2)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_r.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test_r.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['dos', 'dos', 'dos', ..., 'u2r', 'u2r', 'u2r'], dtype=object)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature transform"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Jun 24 16:12:45 2018\n",
    "\n",
    "@author: Nic\n",
    "\"\"\"\n",
    "from sklearn import preprocessing\n",
    "from dataload import load_object, save_object\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "x_train_r, y_train_r = load_object('train_r.dat')\n",
    "scale_list = []\n",
    "for i in range(x_train_r.shape[1]):\n",
    "    if (max(x_train_r[:, i]) > 9):\n",
    "#        print(i, ' - Max : ', max(x_train_r[:, i]))\n",
    "        scale_list.append(i)\n",
    "### remove discrete columns\n",
    "#scale_list.remove(2)\n",
    "#scale_list.remove(7)\n",
    "#scale_list.remove(9)\n",
    "#scale_list.remove(10)\n",
    "#scale_list.remove(16)\n",
    "#scale_list.remove(17)\n",
    "### Min-Max Scalar\n",
    "mms = preprocessing.MinMaxScaler()\n",
    "x_train_r_mms = x_train_r.copy()\n",
    "for i in range(len(scale_list)):\n",
    "    x_train_r_mms[:, [scale_list[i]]] = mms.fit_transform(x_train_r[:, [scale_list[i]]])\n",
    "##print to compare min-max before and after scaling\n",
    "#for i in range(x_train_r.shape[1]):\n",
    "#    print(i, \" : Scaled>> \", min(x_train_r_scaled[:, i]), '-', max(x_train_r_scaled[:, i]), \n",
    "#          ', UnScaled>>', min(x_train_r[:, i]), '-', max(x_train_r[:, i]))\n",
    "\n",
    "#np.savetxt(\"x_train_r_mms.csv\", x_train_r_mms, delimiter=\",\")\n",
    "\n",
    "save_object([x_train_r_mms, y_train_r], 'train_r_scaled.dat')\n",
    "\n",
    "### Binning\n",
    "x_train_r_mms_bin = x_train_r_mms.copy()\n",
    "for i in range(len(scale_list)):\n",
    "    bins = np.array([0.0, \n",
    "                     np.percentile(x_train_r_mms[:, [scale_list[i]]], 25),\n",
    "                     np.percentile(x_train_r_mms[:, [scale_list[i]]], 50),\n",
    "                     np.percentile(x_train_r_mms[:, [scale_list[i]]], 75)\n",
    "                     ])\n",
    "    x_train_r_mms_bin[:, [scale_list[i]]] = np.digitize(x_train_r_mms[:, [scale_list[i]]], bins)\n",
    "#np.savetxt(\"x_train_r_mms_bin.csv\", x_train_r_mms_bin, delimiter=\",\")\n",
    "save_object([x_train_r_mms_bin, y_train_r], 'train_r_bin.dat')\n",
    "'''\n",
    "x_train_r_mms_df = pd.DataFrame(x_train_r_mms)\n",
    "plt.hist(x_train_r_mms_df.iloc[:,0], bins = x_train_r_mms_df.iloc[:,0].value_counts().shape[0])\n",
    "### Robust Scalar\n",
    "rs = preprocessing.RobustScaler()\n",
    "x_train_r_rs = x_train_r.copy()\n",
    "for i in range(len(scale_list)):\n",
    "    x_train_r_rs[:, [scale_list[i]]] = rs.fit_transform(x_train_r[:, [scale_list[i]]])\n",
    "np.savetxt(\"x_train_r_rs.csv\", x_train_r_rs, delimiter=\",\")\n",
    "\n",
    "### Quantile Transformer\n",
    "qt = preprocessing.QuantileTransformer(output_distribution='normal')\n",
    "x_train_r_qt = x_train_r.copy()\n",
    "for i in range(len(scale_list)):\n",
    "    x_train_r_qt[:, [scale_list[i]]] = qt.fit_transform(x_train_r[:, [scale_list[i]]])\n",
    "np.savetxt(\"x_train_r_qt.csv\", x_train_r_qt, delimiter=\",\")\n",
    "\n",
    "### Log Transformer\n",
    "logtf = preprocessing.FunctionTransformer(np.log1p)\n",
    "x_train_r_logtf = x_train_r.copy()\n",
    "for i in range(len(scale_list)):\n",
    "    x_train_r_qt[:, [scale_list[i]]] = logtf.transform(x_train_r[:, [scale_list[i]]])\n",
    "np.savetxt(\"x_train_r_logtf.csv\", x_train_r_logtf, delimiter=\",\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling part two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "x_train, y_train = load_object('train_r.dat')\n",
    "x_test,y_test = load_object('test.dat')\n",
    "x_test = x_test.iloc[:,4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test.fillna(0)\n",
    "# https://stackoverflow.com/questions/13295735/how-can-i-replace-all-the-nan-values-with-zeros-in-a-column-of-a-pandas-datafra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 2), dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = np.asarray(x_test)\n",
    "np.argwhere(np.isnan(x_test))\n",
    "# with thanks to... \n",
    "# https://stackoverflow.com/questions/37754948/how-to-get-the-indices-list-of-all-nan-value-in-numpy-array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_scaler = StandardScaler()\n",
    "m_scaler = MinMaxScaler()\n",
    "ss_train = s_scaler.fit_transform(x_train)\n",
    "mm_train = m_scaler.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_test = s_scaler.fit_transform(x_test)\n",
    "mm_test = m_scaler.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3086687       dos\n",
       "578347        dos\n",
       "3732971       dos\n",
       "3994439       dos\n",
       "813098     normal\n",
       "Name: attack_type, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = y_test.iloc[:,1]\n",
    "y_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "initial set is logistic only because i got stuck preprocessing test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_train, y_train = load_object('train_r_bin.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mms_train, y_train = load_object('train_r_scaled.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# basic logistic regression model\n",
    "from sklearn import linear_model\n",
    "\n",
    "logit = linear_model.LogisticRegression() \n",
    "logit.fit(x_train, y_train)\n",
    "# note: x_train, y_train = load_object('train_r.dat') -- defined earlier. unscaled.\n",
    "#logit.coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 of the logistic model with no scaling (train set) is  0.58341\n"
     ]
    }
   ],
   "source": [
    "print(\"R2 of the logistic model with no scaling (train set) is \", logit.score(x_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 of the logistic model with Standard Scaling (train set) is  0.9348\n"
     ]
    }
   ],
   "source": [
    "logit.fit(ss_train,y_train)\n",
    "print(\"R2 of the logistic model with Standard Scaling (train set) is \", logit.score(ss_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 of the logistic model with MinMax Scaling (train set) is  0.91659\n"
     ]
    }
   ],
   "source": [
    "logit.fit(mm_train,y_train)\n",
    "print(\"R2 of the logistic model with MinMax Scaling (train set) is \", logit.score(mm_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 of the logistic model with MinMax Scaling Nic's way (train set) is  0.9172\n"
     ]
    }
   ],
   "source": [
    "logit.fit(mms_train,y_train)\n",
    "print(\"R2 of the logistic model with MinMax Scaling Nic's way (train set) is \", logit.score(mm_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 of the logistic model with MinMax Scaler (test set) is  0.9727076375335821\n"
     ]
    }
   ],
   "source": [
    "print(\"R2 of the logistic model with MinMax Scaler (test set) is \", logit.score(mm_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 of the logistic model with Standard Scaler (test set) is  0.7375245996684658\n"
     ]
    }
   ],
   "source": [
    "print(\"R2 of the logistic model with Standard Scaler (test set) is \", logit.score(ss_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 of the logistic model with no scaling (test set) is  0.8044499881594956\n"
     ]
    }
   ],
   "source": [
    "print(\"R2 of the logistic model with no scaling (test set) is \", logit.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 of the logistic model with MinMax Scaling (test set) is  0.8887298813499808\n"
     ]
    }
   ],
   "source": [
    "print(\"R2 of the logistic model with MinMax Scaling (test set) is \", logit.score(mm_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 of the logistic model with Standard Scaling (test set) is  0.70620850719821\n"
     ]
    }
   ],
   "source": [
    "print(\"R2 of the logistic model with Standard Scaling (test set) is \", logit.score(ss_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
